# Reflexiones finales

Luego de haber finalizado este proyecto de clustering usando algoritmos de aprendizaje no supervisado en el ámbito del Machine Learning, puedo rescatar los siguientes aprendizajes:

Ahondo aún más en el lenguaje Python, esta vez aprendiendo nuevos algoritmos de clustering que antes me resultaban desconocidos (como DBSCAN y GMM). A su vez, pude cumplir a cabalidad con las etapas de un proyecto de análisis y ciencia de datos, en este caso desde el preprocesamiento y limpieza inicial, análisis exploratorio, aplicación de algoritmos de clustering y perfilamiento final.

Algo destacable de este proyecto fue el tratamiento de los "outliers" (valores atípicos). Si bien uno podría pensar en eliminar estos registros directamente al no representar un comportamiento común, esta vez apliqué un criterio riguroso para su exploración e imputación en caso de ser necesario. Al explorar las distribuciones de frecuencia de las variables numéricas, se podía notar que habían colas (izquierdas y derechas) que podían entorpecer la implementación posterior de los algoritmos de clustering. Sin embargo, estos valores que se encontraban sobre el 95% de su distribución, correspondían en total al 37% del dataset, porcentaje importante del cual no es posible eliminar sin razón. Es por ello, que algunos outliers se imputaron en base a la mediana agrupando por ingreso, otros no se trataron porque podían corresponder a un comportamiento único de determinado cluster, y otros se imputaron por la media del 5% superior que los represetaba. De esta forma, las distribuciones de estas variables se suavizaron en comportamiento sin perder ningún dato de nuestro dataframe. 

A su vez, ahondo en algoritmos de clustering, algunos conocidos, otros nuevos. KMeans tiene el supuesto de agrupar datos que tienen comportamiento esférico, lo cual no se daba en este caso. DBSCAN busca densidad uniforme en los datos para agrupar a los vecinos más cercanos, lo cual tampoco se dió ya que los datos tenían un comportamiento más disperso. GMM intentó agrupar mediante el supuesto de que cada variable corresponde a una distribución normal, lo cual de antemano sabíamos que no se cumplía para todas las variables. Finalmente decidí quedarme con Clustering Jerárquico Aglomerativo,esto por entregar un equilibrio entre mejores métricas de evaluación con visualización y coherencia con el negocio.

Finalmente, puedo destacar que la finalización de este proyecto logra una mejoría en mi entendimiento del problema de Clustering en Machine Learning, sus aplicaciones prácticas, y los desafíos respecto a las demás temáticas del Machine Learning.
